# -*- coding: utf-8 -*-
"""Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xfcgWJUXFn-5trd7mtMYiTnXZPZjZAs5

**Maddy Arend**

**QMSS 495 Final Project**

This project aims to answer the question of whether a predictive model can be built to accurately predict whether a U.S. resident's annual income is above $50,000. Using data from the 1994 U.S. Census consisting of 32,561 observations and 14 features, I build three predictive models with machine learning techniques to attempt to answer this question and interpret the broader implications of the models' findings.
"""

# import libraries
import pandas as pd
import numpy as np

# read in census income data
adult = pd.read_csv("adult.data", header=None, delimiter=",", skipinitialspace=True, na_values="?")

# view data
adult.head()

# get shape of data
adult.shape

# get counts of observations in each income category
adult.iloc[:,14].value_counts()

"""There is a category imbalance in the data, with 76% of the observations having incomes less than or equal to $50,000.

# Data Preprocessing
"""

# create list of column names
column_names = ["age", "workclass", "fnlwgt", "education", "education_num", "marital_status",
    "occupation", "relationship", "race", "sex", "capital_gain", "capital_loss",
    "hours_per_week", "native_country", "income"]

# rename columns in data table with variable names
adult.columns = column_names

"""I decided to drop fnlwgt from the datatable as I beleive the relative percentage of the U.S. population that is represented by a given person in the dataset is not predictive of whether or not that person's income is above $50,000. This can be seen in the scatterplot below."""

# create scatterplot of fnlwgt vs. income
import matplotlib.pyplot as plt

# convert income variable to binary
adult['income_numeric'] = adult['income'].apply(lambda x: 1 if x == '>50K' else 0)

# create scatterplot
plt.figure(figsize=(10, 6))
plt.scatter(adult['fnlwgt'], adult['income_numeric'], alpha=0.5)
plt.xlabel('fnlwgt')
plt.ylabel('Income (0 = <=50K, 1 = >50K)')
plt.title('Scatter Plot of fnlwgt vs Income')
plt.show()

# drop fnlwgt from datatable (and get rid of income_numeric, will encode with other variables later on)
adult = adult.drop(columns = ['fnlwgt', 'income_numeric'])

# examine the dataset for missing values
adult.isna().sum()

"""Workclass: 5.64% of observations have missing values

Occupation: 5.66% of observations have missing values

Native_country: 1.79% of observations have missing values

Since a relatively small portion of observations have missing values, it is reasonable to continue with my analysis by imputing most frequent values of each variable into missing values.
"""

# handle missing values
from sklearn.impute import SimpleImputer

# all variables with missing values are categorical, use most frequent value to fill in missing values
imputer_cat = SimpleImputer(strategy='most_frequent')

# apply imputer to workclass, occupation, and native_country
adult['workclass'] = imputer_cat.fit_transform(adult[['workclass']]).ravel()
adult['occupation'] = imputer_cat.fit_transform(adult[['occupation']]).ravel()
adult['native_country'] = imputer_cat.fit_transform(adult[['native_country']]).ravel()

# what category will be inputed for missing values in workclass?
adult['workclass'].value_counts()

"""Private is the most frequent value for workclass in the dataset, so it will be imputed for missing values in the column."""

# what category will be inputed for missing values in occupation?
adult['occupation'].value_counts()

"""Prof-specialty is the most frequent value for occupation in the dataset, so it will be imputed for missing values in the column."""

# what category will be inputed for missing values in native_country?
adult['native_country'].value_counts()

"""United States is the most frequent value for native country in the dataset, so it will be imputed for missing values in the column."""

# check to make sure all missing values have been handled
adult.isna().sum()

# check datatype of each variable
adult.dtypes

# create copy of datatable before encoding
adult_copy = adult

# encode categorical variables

# create a list of categorical variables
cat_variables = ["workclass", "education", "marital_status", "occupation", "relationship", "race", "sex",
"native_country", "income"]

# use OneHotEncoder to convert categorical variables to numerical format
from sklearn.preprocessing import OneHotEncoder
encoder = OneHotEncoder(drop='first', sparse_output=False)
encoded_cats = pd.DataFrame(encoder.fit_transform(adult[cat_variables]))

# add encoded columns to the original dataframe
encoded_cats.columns = encoder.get_feature_names_out(cat_variables)
adult = pd.concat([adult, encoded_cats], axis=1)

# drop original categorical variables from dataframe
adult.drop(columns=cat_variables, inplace=True)

"""Since I will be building an SVM model later in my analysis, I will scale my numerical variables using standard scaler. The distance between data points affects the decision boundary SVM chooses, and since my data contains variables of different units, I predict that using standard scaler will increase the accuracy of my model. For the random forest and gradient boosted models I will construct, standardization of my data is not important since these algorithms are tree-based and partition the data based on values of each feature."""

# apply scaling using standard scaler to numerical variables
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaled_values = scaler.fit_transform(adult[['age', 'capital_gain', 'education_num', 'capital_loss', 'hours_per_week']])

# create a new dataframe to hold the scaled values
adult_scaled = adult.copy()

# assign the scaled values to the new dataframe
adult_scaled[['age', 'capital_gain', 'education_num', 'capital_loss', 'hours_per_week']] = scaled_values

# view processed data
adult_scaled.head()

"""# Random Forest

The first model I will construct is a random forest, which is an ensemble algorithm that builds multiple decision trees to predict the class of an observation. I chose this method because random forests can handle large datasets and are robust to overfitting. Instead of creating a simple decision tree, a random forest introduces randomization in its construction to better predict the class of each observation. Intuitively, tree-based methods are easy to understand for classification problems, which I think is important when using machine learning to gain insights regarding social science issues.
"""

# import random forest classifier
from sklearn.ensemble import RandomForestClassifier

# split data into training and testing
from sklearn.model_selection import train_test_split

# define target and features
X= adult_scaled.drop(columns=['income_>50K'])
y = adult_scaled['income_>50K']

# split data
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

"""In order to build different decision trees using the same set of data, bootstrapping and feature randomization must be introduced. Otherwise, a random forest built without bootstrapping and feature randomization will yield the same results as constructing a singular decision tree."""

# build random forest with bootstrapping and feature randomization, default settings otherwise
forest = RandomForestClassifier(n_estimators=100, bootstrap=True, random_state=0)
forest.fit(X_train, y_train)

# print accuracy score of random forest model
print('Accuracy Score on Test Set:', forest.score(X_test, y_test))
print('Accuracy Score on Training Set:', forest.score(X_train, y_train))

"""These results indicate the random forest model is overfit to the training data. I will now use randomsearchCV to find the best combination of hyperparameters to improve the accuracy of the model on the test data. I chose to use randomsearchCV instead of gridsearchCV because completing a gridsearch would take a significant amount of time and computing power to check 5 parameters."""

# tune hyperparameters using randomizedsearchCV
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import accuracy_score

# initialize random forest model
tuned_forest = RandomForestClassifier(random_state=0, bootstrap=True)

# define paramaters to test
param_distributions = {
    'n_estimators': [50, 100, 150, 200],
    'max_depth': [None, 1, 2, 3, 4, 5],
    'max_features' : [None, 1, 5, 10, 15],
    'min_samples_split' : [2, 10, 15],
    'min_samples_leaf' : [None, 5, 10, 15]
}

# peform randomizedsearchCV to find best hyperparameters
random_search = RandomizedSearchCV(estimator=tuned_forest, param_distributions=param_distributions,
                                   n_iter=10, cv=5, random_state=0, n_jobs=-1, verbose=2)
random_search.fit(X_train, y_train)

# get the best parameters and model
best_params_rand = random_search.best_params_
best_model_rand = random_search.best_estimator_

# print the best parameters from RandomizedSearchCV
print(f"Best Parameters (RandomizedSearchCV): {best_params_rand}")

# evaluate the model with the test data
y_pred_rand = best_model_rand.predict(X_test)
accuracy_rand = accuracy_score(y_test, y_pred_rand)
print(f"Test Accuracy (RandomizedSearchCV): {accuracy_rand:.4f}")

# final model using hyperparameters from randomizedsearchCV
final_forest = RandomForestClassifier(random_state=0, bootstrap=True, n_estimators = 50, min_samples_split=10, min_samples_leaf=5, max_depth=5, max_features=None,)
final_forest.fit(X_train, y_train)

# print accuracy score of final model on test and training data
print('Accuracy Score on Test Set:', final_forest.score(X_test, y_test))
print('Accuracy Score on Training Set:', final_forest.score(X_train, y_train))

"""This model is slightly more accurate than the original model without hyperparameter tuning, and is no longer overfit to the training data.

Using the final model with the tuned hyperparameters, I will now do stratified cross-validation to ensure that the accuracy of my model is not due to the way my data was split for training and testing.
"""

# get cross validation scores from stratified validation
from sklearn.model_selection import cross_val_score
print("Cross-validation scores:\n{}".format(
      cross_val_score(final_forest, X, y , cv=5)))

"""To visualize the predictions of the final random forest model, I will create a confusion matrix and a heatmap. This gives an evaluation of the model beyond accuracy score, which only considers the total number of correct predictions of the model."""

# create confusion matrix
from sklearn.metrics import confusion_matrix

# predict y values using model
y_pred_forest = final_forest.predict(X_test)

# create confusion matrix
cm_forest = confusion_matrix(y_test, y_pred_forest)
print(cm_forest)

# create a heat map

#import required packages
import seaborn as sns
import matplotlib.pyplot as plt

# create visualization
sns.heatmap(cm_forest, annot=True, cmap='Blues', cbar=True)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

"""My model yielded 1061 true positives, 5882 true negatives, 277 false positives, and 921 false negatives.

Note that the model predicts 84.19% of the incomes to be below 50,000 dollars, likely due to the inbalance in the data.
"""

# random forest evaluation metrics
from sklearn.metrics import classification_report

# print classification report
forest_report = classification_report(y_test, y_pred_forest)
print("Classification Report:\n", forest_report)

"""**Accuracy Score:** The random forest model was able to predict 85% of the incomes in the test set correctly.

**Precision Score**: The precision score of the random forest model was 79%, meaning, out of all of the observations predicted by the model to have incomes over 50,000 dollars, 79% of them actually had incomes above 50,000 dollars (true positives).

**Recall Score:** The recall score of the random forest model was 54%, which means that the model correctly predicted the incomes of 54% of the observations which had incomes above 50,000 dollars.

**F1 Score**: The F1 score of the random forest model, which is a harmonic mean of the precision recall scores, was 64%.

"""

# plot AUC-ROC curve
from sklearn.metrics import roc_curve, roc_auc_score

# get predicted probabilities from positive class (income over 50k)
y_pred_proba = final_forest.predict_proba(X_test)[:, 1]

# compute ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)

# compute AUC score
auc_score = roc_auc_score(y_test, y_pred_proba)

# plot the ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f"AUC = {auc_score:.2f}")
plt.plot([0, 1], [0, 1], 'k--', label="Random Classifier")
plt.title("ROC Curve of Random Forest Model")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend(loc="lower right")
plt.grid()
plt.show()

"""The ROC curve and corresponding AUC of 0.90 show that the random forest model has high predictive ability and is a much more powerful model than one which randomly guesses the class of the observations (represented by the dashed line). Across all classification thresholds, the random forest model does a good job of effectively identifying true positive cases (incomes above 50k) while keeping false positives (incorrectly classifying observations where the true income is below 50k) realtively low.

# Gradient Boosted

I will now use a different type of ensemble tree-based algorithm: gradient boosted. I will aim to see if introducing a learning rate to the predictive model can increase its accuracy.
"""

# import gradient boosting classifier
from sklearn.ensemble import GradientBoostingClassifier

# split data into training and testing
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

# build gradient boosted model with default settings (100 trees of maximum depth 3 and a learning rate of 0.1)
gbrt = GradientBoostingClassifier(random_state=0)
gbrt.fit(X_train, y_train)

# print accuracy score of gradient boosted model
print('Accuracy Score on Test Set:', gbrt.score(X_test, y_test))
print('Accuracy Score on Training Set:', gbrt.score(X_train, y_train))

"""The gradient boosted model is not overfitted or underfitted to the training data. I will now use randomsearchCV to tune the hyperparameters of the model to see if I can increase it's accuracy. Again, I use randomized search instead of grid search because of the computing power and time it would take to test 3 parameters."""

# perform hyperparameter tuning for the gradient boosted model

# initialize gradient boosted model
tuned_gbrt = GradientBoostingClassifier(random_state=0)

# define paramaters to test
param_distributions = {
    'n_estimators': np.arange(50, 201, 50), # test 50, 100, 150, and 200 estimators
    'learning_rate': np.linspace(0.01, 0.3, 5), # test learning rates 0.01, 0.0825, 0.155, 0.2275, 0.3
    'max_depth': np.arange(3, 6)} # test max depths 3, 4, 5

# peform randomizedsearchCV to find best hyperparameters
random_search = RandomizedSearchCV(estimator=tuned_gbrt, param_distributions=param_distributions,
                                   n_iter=10, cv=5, random_state=0, n_jobs=-1, verbose=2)
random_search.fit(X_train, y_train)

# get the best parameters and model
best_params_rand = random_search.best_params_
best_model_rand = random_search.best_estimator_

# print the best parameters from RandomizedSearchCV
print(f"Best Parameters (RandomizedSearchCV): {best_params_rand}")

# evaluate the model with the test data
y_pred_rand = best_model_rand.predict(X_test)
accuracy_rand = accuracy_score(y_test, y_pred_rand)
print(f"Test Accuracy (RandomizedSearchCV): {accuracy_rand:.4f}")

# build final model using parameters from randomized search
final_gbrt = GradientBoostingClassifier(random_state=0, n_estimators=150, max_depth=5, learning_rate=0.08249999999999999)
final_gbrt.fit(X_train, y_train)

# print accuracy score of final model on test and training data
print('Accuracy Score on Test Set:', final_gbrt.score(X_test, y_test))
print('Accuracy Score on Training Set:', final_gbrt.score(X_train, y_train))

"""The tuned gradient boosted model is more accurate on both the training and test data sets than the model using the default settings.

Using the final model with the tuned hyperparameters, I will now do stratified cross-validation to ensure that the accuracy of my model is not due to the way my data was split for training and testing.
"""

# get cross validation scores from stratified validation
print("Cross-validation scores:\n{}".format(
      cross_val_score(final_gbrt, X, y , cv=5)))

"""To visualize the predictions of the final gradient boosted model, I will create a confusion matrix and a heatmap. This gives an evaluation of the model beyond accuracy score, which only considers the total number of correct predictions of the model."""

# create confusion matrix

# predict y values using test data
y_pred_gbrt = final_gbrt.predict(X_test)

# create confusion matrix
cm_gbrt = confusion_matrix(y_test, y_pred_gbrt)
print(cm_gbrt)

# create heatmap
sns.heatmap(cm_gbrt, annot=True, cmap='Blues', cbar=True)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

"""The gradient boosted model yielded 1305 true positives, 5776 true negatives, 383 false positives, and 677 false negatives.

Note that the model predicts 79.27% of the incomes to be below 50,000 dollars, likely due to the inbalance in the data.
"""

# get evaluation metrics of gradient boosted model
gbrt_report = classification_report(y_test, y_pred_gbrt)
print("Classification Report:\n", gbrt_report)

"""**Accuracy Score:** The gradient boosted model was able to predict 87% of the incomes in the test set correctly.

**Precision Score**: The precision score of the gradient boosted model was 77%, meaning, out of all of the observations predicted by the model to have incomes over 50,000 dollars, 77% of them actually had incomes above 50,000 dollars (true positives).

**Recall Score:** The recall score of the gradient boosted model was 66%, which means that the model correctly predicted the incomes of 66% of the observations which had incomes above 50,000 dollars.

**F1 Score**: The F1 score of the gradient boosted model, which is a harmonic mean of the precision recall scores, was 71%.

"""

# plot AUC-ROC curve

# get predicted probabilities from positive class (income over 50k)
y_pred_proba = final_gbrt.predict_proba(X_test)[:, 1]

# compute ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)

# compute AUC score
auc_score = roc_auc_score(y_test, y_pred_proba)

# plot the ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f"AUC = {auc_score:.2f}")
plt.plot([0, 1], [0, 1], 'k--', label="Random Classifier")
plt.title("ROC Curve of Gradient Boosted Model")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend(loc="lower right")
plt.grid()
plt.show()

"""The ROC curve and corresponding AUC of 0.93 show that the gradient boosted model has high predictive ability and is a much more powerful model than one which randomly guesses the class of the observations (represented by the dashed line). It is also slightly more effective than the random forest model (AUC of 0.93 > 0.90). Across all classification thresholds, the gradient boosted model does a good job of effectively identifying true positive cases (incomes above 50k) while keeping false positives (incorrectly classifying observations where the true income is below 50k) realtively low.

# Support Vector Machine

Now that I have built two models which are tree-based, I will build a model using an algorith that instead constructs a decision boundary to make predictions on the data (SVM). I will set out to see if using a decision-boundary based model instead of a decision tree based model will create a model with higher predictive power. SVM introduces a regularization level, which aims to prevent overfitting of the model to the training data by introducing a penalty on the coefficients of the features based on their importance in predicting the value of the target variable.
"""

# import SVM classifier
from sklearn.svm import SVC

# split data into training and testing
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

# build SVM model with default settings (regularization parameter C = 1.0)
svm = SVC(kernel = 'linear', C = 1.0, probability=True)
svm.fit(X_train, y_train)

# print accuracy score of SVM model
print('Accuracy Score on Test Set:', svm.score(X_test, y_test))
print('Accuracy Score on Training Set:', svm.score(X_train, y_train))

"""The SVM model using regularization level 1.0 is not overfit to the training set. I will now use hyperparameter tuning to see if I can improve the accuracy of the model. Since there is only one parameter being tuned, I will use grid search instead of randomized search."""

# perform hyperparameter tuning using grid search for the SVM model to test different levels of C
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score

# initialize gradient boosted model
tuned_svm = SVC(kernel = 'linear')

# define paramaters to test
param_distributions = {
    'C': [0.01, 0.1, 1, 10, 100]}

# peform grid search to find best hyperparameters
grid_search = GridSearchCV(estimator=tuned_svm, param_grid=param_distributions, cv=3, n_jobs=-1, verbose=2)
grid_search.fit(X_train, y_train)

# Get the best parameters and model
best_params = grid_search.best_params_
best_model = grid_search.best_estimator_

# print the best parameters from gridsearch
print(f"Best Parameters (GridSearchCV): {best_params}")

# evaluate the model with the test data
y_pred_rand = best_model.predict(X_test)
accuracy_rand = accuracy_score(y_test, y_pred_rand)
print(f"Test Accuracy (GridSearchCV): {accuracy_rand:.4f}")

"""The grid search suggests using regularization level C = 10 (less regularization than in the default model). However, this yields a slightly less accurate model than when using C = 1.0, so I will continue my analysis using C = 1.0."""

# rename original model to final model
final_svm = svm

# print accuracy scores for final svm model
print('Accuracy Score on Test Set:', final_svm.score(X_test, y_test))
print('Accuracy Score on Training Set:', final_svm.score(X_train, y_train))

"""Using the final model with the tuned hyperparameters, I will now do stratified cross-validation to ensure that the accuracy of my model is not due to the way my data was split for training and testing."""

# get cross validation scores from stratified validation
from sklearn.model_selection import cross_val_score
print("Cross-validation scores:\n{}".format(
      cross_val_score(final_svm, X, y , cv=5)))

# create confusion matrix

# predict y values using test data
y_pred_svm = final_svm.predict(X_test)

# create confusion matrix
cm_svm = confusion_matrix(y_test, y_pred_svm)
print(cm_svm)

# create heatmap
sns.heatmap(cm_svm, annot=True, cmap='Blues', cbar=True)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

"""The SVM model yielded 1132 true positives, 5772 true negatives, 387 false positives, and 850 false negatives.

Note that the model predicts 81.34% of the incomes to be below 50,000 dollars, likely due to the inbalance in the data.
"""

# get evaluation metrics of SVM model
svm_report = classification_report(y_test, y_pred_svm)
print("Classification Report:\n", svm_report)

"""**Accuracy Score:** The SVM model was able to predict 85% of the incomes in the test set correctly.

**Precision Score**: The precision score of the SVM model was 75%, meaning, out of all of the observations predicted by the model to have incomes over 50,000 dollars, 75% of them actually had incomes above 50,000 dollars (true positives).

**Recall Score:** The recall score of the SVM model was 57%, which means that the model correctly predicted the incomes of 57% of the observations which had incomes above 50,000 dollars.

**F1 Score**: The F1 score of the SVM model, which is a harmonic mean of the precision recall scores, was 65%.

"""

# plot AUC-ROC curve

# get predicted probabilities from positive class (income over 50k)
y_pred_proba = final_svm.predict_proba(X_test)[:, 1]

# compute ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)

# compute AUC score
auc_score = roc_auc_score(y_test, y_pred_proba)

# plot the ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f"AUC = {auc_score:.2f}")
plt.plot([0, 1], [0, 1], 'k--', label="Random Classifier")
plt.title("ROC Curve of SVM Model")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend(loc="lower right")
plt.grid()
plt.show()

"""The ROC curve and corresponding AUC of 0.90 show that the SVM model has high predictive ability and is a much more powerful model than one which randomly guesses the class of the observations (represented by the dashed line). It is slightly less effective than the gradient boosted model (AUC of 0.93 > 0.90), but has the same AUC as the random forest model. Across all classification thresholds, the SVM model does a good job of effectively identifying true positive cases (incomes above 50k) while keeping false positives (incorrectly classifying observations where the true income is below 50k) realtively low.

# Conclusions

Out of the three models I built, the gradient boosted model seems like the best predictor of income category for the dataset. It has the highest accuracy score, recall score, F1 score, and AUC score, as well as the second highest precision score (0.02 points less than the precision score of the random forest model).

I will now look at feature importances in the gradient boosted model.
"""

# print top 20 most important features in the model

# extract feature importances from the gradient boosted model
importances = final_gbrt.feature_importances_

# get the feature names from the adult dataframe
features = X.columns

# create a dataframe to hold feature names and their importance values
importance_df = pd.DataFrame({
    'Feature': features,
    'Importance': importances
})

# sort the dataframe by importance in descending order
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# limit to the top 20 most important features
top_20_importance_df = importance_df.head(20)

# plot the top 20 feature importances using seaborn
plt.figure(figsize=(10, 8))

# create barplot
sns.barplot(x='Importance', y='Feature', data=top_20_importance_df)
plt.title('Top 20 Feature Importances from Gradient Boosting Model')
plt.xlabel('Feature Importance')
plt.ylabel('Feature')
plt.show()

"""The most important predictors of income category in the dataset, as determined by the gradient boosted model, were: being married to a civilian spouse, capital gain, education number, capital loss, age, and hours worked per week.

Let's look at differences between the different income groups in each of these factors.
"""

# pie charts of marriage status by income group
import matplotlib.pyplot as plt

# filter the dataset by income group
adult_under_50K = adult_copy[adult_copy['income'] == '<=50K']
adult_over_50K = adult_copy[adult_copy['income']== '>50K']

# group by marriage status and count occurrences
marriage_status_counts_under_50K = adult_under_50K['marital_status'].value_counts()
marriage_status_counts_over_50K = adult_over_50K['marital_status'].value_counts()

# define a consistent color palete between charts
all_marital_statuses = pd.concat([marriage_status_counts_under_50K, marriage_status_counts_over_50K]).index.unique()
colors = plt.cm.Paired(range(len(all_marital_statuses)))
marital_status_to_color = {status: colors[i] for i, status in enumerate(all_marital_statuses)}

# plot pie charts

# pie chart for people with income <= 50k
plt.figure(figsize=(8, 8))
plt.pie(marriage_status_counts_under_50K,
        autopct='%1.1f%%',
        startangle=90,
        colors=[marital_status_to_color[status] for status in marriage_status_counts_under_50K.index])
plt.legend(marriage_status_counts_under_50K.index, loc="best", fontsize=10)
plt.title('Marriage Status Distribution Among People with Income <= 50k')
plt.axis('equal')
plt.show()

# pie chart for people with income > 50k
plt.figure(figsize=(8, 8))
plt.pie(marriage_status_counts_over_50K,
        autopct='%1.1f%%',
        startangle=90,
        colors=[marital_status_to_color[status] for status in marriage_status_counts_over_50K.index])
plt.legend(marriage_status_counts_over_50K.index, loc="best", fontsize=10)
plt.title('Marriage Status Distribution Among People with Income > 50k')
plt.axis('equal')
plt.show()

"""Among people with incomes under 50k, 33.5% are married to a civilian spouse, and among people with incomes over 50k, 85.3% are married to a civilan spouse. This indicates a correlation between being married to a civilan spouse and having a higher income."""

# bar chart of average capital gain by income group

# group the data by income and calculate the average capital gain for each group
avg_capital_gain = adult_copy.groupby('income')['capital_gain'].mean()

# plot bar chart
plt.figure(figsize=(8, 6))
ax = avg_capital_gain.plot(kind='bar', color=['skyblue', 'salmon'])
for i, value in enumerate(avg_capital_gain):  # add average value labels to bars
    ax.text(i, value + 20, f'{value:.2f}', ha='center', va='bottom', fontsize=12)
plt.title('Average Capital Gain by Income Group')
plt.xlabel('Income Group')
plt.ylabel('Average Capital Gain')
plt.xticks(rotation=0)
plt.show()

"""The average capital gain among people with incomes below 50k is 148.75 dollars, wheras the average capital gain among people with incomes above 50k is 4,006.14 dollars. This indicates a correlation between making investments in financial assets with significant positive returns and having a higher income. This may be because people with higher incomes have more financial freedom to invest in assets such as stocks, bonds, and real estate."""

# bar chart of average education num by income group

# group the data by income and calculate the average education number for each group
avg_educ_num = adult_copy.groupby('income')['education_num'].mean()

# plot bar chart
plt.figure(figsize=(8, 6))
ax = avg_educ_num.plot(kind='bar', color=['skyblue', 'salmon'])
for i, value in enumerate(avg_educ_num):  # add average value labels to bars
    ax.text(i, value + 0.04, f'{value:.2f}', ha='center', va='bottom', fontsize=12)
plt.title('Average Years of Education by Income Group')
plt.xlabel('Income Group')
plt.ylabel('Average Years of Education')
plt.xticks(rotation=0)
plt.show()

"""The average years of education among people with incomes under 50k is 9.60, wheras people with incomes above 50k have an average of 11.61 years of schooling. This indicates an association between having more years of education and having a higher income."""

# bar chart of average capital loss by income group

# group the data by income and calculate the average capital loss for each group
avg_capital_loss = adult_copy.groupby('income')['capital_loss'].mean()

# plot bar chart
plt.figure(figsize=(8, 6))
ax = avg_capital_loss.plot(kind='bar', color=['skyblue', 'salmon'])
for i, value in enumerate(avg_capital_loss):  # add average value labels to bars
    ax.text(i, value + 0.04, f'{value:.2f}', ha='center', va='bottom', fontsize=12)
plt.title('Average Capital Loss by Income Group')
plt.xlabel('Income Group')
plt.ylabel('Average Capital Loss')
plt.xticks(rotation=0)
plt.show()

"""The average capital loss is 53.14 dollars among people with incomes below 50k, whereas peopel with incomes above 50k have an average of 195 dollars in capital losses. Similar to capital gains, this relationship is likely due to the increased ability of people with higher incomes to invest in financial assets, leading to both higher potential gains and losses."""

# bar chart of average age by income group

# group the data by income and calculate the average age for each group
avg_age = adult_copy.groupby('income')['age'].mean()

# plot bar chart
plt.figure(figsize=(8, 6))
ax = avg_age.plot(kind='bar', color=['skyblue', 'salmon'])
for i, value in enumerate(avg_age):  # add average value labels to bars
    ax.text(i, value + 0.04, f'{value:.2f}', ha='center', va='bottom', fontsize=12)
plt.title('Average Age by Income Group')
plt.xlabel('Income Group')
plt.ylabel('Average Age')
plt.xticks(rotation=0)
plt.show()

"""People with incomes below 50k have an average age of 36.78, and people with incomes above 50k have an average age of 44.25. This indicates a relationship between being older, and therefore being at a later point in a career, and having a higher income."""

# bar chart of average hours per week by income group

# group the data by income and calculate the average hours per week for each group
avg_hours = adult_copy.groupby('income')['hours_per_week'].mean()

# plot bar chart
plt.figure(figsize=(8, 6))
ax = avg_hours.plot(kind='bar', color=['skyblue', 'salmon'])
for i, value in enumerate(avg_hours):  # add average value labels to bars
    ax.text(i, value + 0.04, f'{value:.2f}', ha='center', va='bottom', fontsize=12)
plt.title('Average Hours Worked Per Week by Income Group')
plt.xlabel('Income Group')
plt.ylabel('Average Hours Worked Per Week')
plt.xticks(rotation=0)
plt.show()

"""People with incomes below 50k work 38.84 hours per week on average, while people with incomes above 50k work an average of 45.47 hours per week. This indicates an association between working longer hours and having a higher income. People who work more hours may earn overtime pay, or this could be an indication of working a job which requires long hours but is higher-paying, such as jobs in the law, medical, or financial fields.

These factors can be important in policy decisions made regarding which demographics of people may qualify for tax breaks or government assistance. As seen in the analysis above, people who are married to a civilian spouse, have investments in financial assets, have more education, are older, and work more hours per week are more likely to have an income above 50,000 dollars. This means people outside of these groups may be more sucseptible to facing financial hardship and should be the targets for tax breaks or government assistance programs such as stimulus checks and food stamps.

Further research into the exact thresholds that should be used for numeric variables such as years of education, age, and value of financial assets when determining government assistance eligibility is necessary.

# Unsupervised Learning Extension

As an extension of my analysis above, I will now use principle component analysis to reduce the dimensionality of my data and reconstruct the gradient boosted model using the transformed data to try to improve the accuracy of the model.
"""

# apply pca to scaled data
from sklearn.decomposition import PCA

pca = PCA(n_components=2) # create two principal components
X_train_pca = pca.fit_transform(X_train)
X_test_pca = pca.transform(X_test)

# train gradient boosted classifier with tuned hyperparameters on transformed data
classifier = GradientBoostingClassifier(random_state=0, n_estimators=150, max_depth=5, learning_rate=0.08249999999999999)
classifier.fit(X_train_pca, y_train.values.ravel())

# make predictions on the test set
y_pred_pca = classifier.predict(X_test_pca)

# get accuracy score of gradient boosted model using pca data
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(y_test, y_pred_pca)
print(f"Accuracy: {accuracy:.2f}")

# print classification report for gradient boosted model using pca data
from sklearn.metrics import classification_report
gbrt_pca_report = classification_report(y_test, y_pred_pca)
print("Classification Report:\n", gbrt_pca_report)

"""Reducing the dimensionality of my features did not help improve the accuracy of my model. In addition, my precision, recall, and F1 scores were significantly lower using PCA compared to my final gradient boosted model without PCA. This could be because my original model only used 13 features, and reducing this to two features removed a significant amount of predictive power from the model. PCA would likely work better on a dataset with more features."""